{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9d3c5b-7568-45d7-83ad-dee89234ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings # updated code\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "import textwrap\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d615728a-4c58-4a0d-9a06-5deeb0bd7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_from_youtube_video_url(video_url):\n",
    "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
    "    transcript = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents(transcript)\n",
    "\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba1575e-a106-441a-997d-93a706caa6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=fNxaJsNG3-s&list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S\"\n",
    "db = create_db_from_youtube_video_url(video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2697d9-edc5-4fa7-8327-c33d82613075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAURENCE MORONEY: Through this series so far, you've been learning the basics of NLP using TensorFlow.\n",
      "\n",
      "You saw how to tokenize and then sequence text, preparing it to train neural networks.\n",
      "\n",
      "You saw how sentiment in text can be represented with embeddings and how the semantics of text over long stretches might be learned using recurrent neural networks and LSTMs.\n",
      "\n",
      "In this video, we'll put all of that together into a fun scenario.\n",
      "\n",
      "We'll create a model and train it on the lyrics of traditional Irish songs.\n",
      "\n",
      "From that, you'll see then if it can write its own poetry using those words.\n",
      "\n",
      "Let's look at the steps involved.\n",
      "\n",
      "First of all, this is our text.\n",
      "\n",
      "Within the entire corpus are the lyrics to lots of Irish songs.\n",
      "\n",
      "One of them, \"Lanigan's Ball,\" is listed here, and you can see these words have a very distinctive style.\n",
      "\n",
      "If we were to read them in, we could do it something like this.\n",
      "\n",
      "And for simplicity, I'll just use this one song for now.\n",
      "\n",
      "It's stored as a single string with slash n's to give new lines.\n",
      "\n",
      "That, I can then break into a number of sentences by splitting the string by that new line character, and this will form my corpus of text.\n",
      "\n",
      "Later, you'll see how to change it to read the full corpus off of disk, but the methods will be exactly the same.\n",
      "\n",
      "I can then fit my tokenizer to the corpus to get a word index.\n",
      "\n",
      "As I'm using an out of vocabulary token, I'll add 1 to the length of the word index just to cater for that.\n",
      "\n",
      "Now, you might wonder, why not just encode with an out of vocabulary token? There's a subtle difference here when generating text over the previous scenario when we were classifying text.\n",
      "\n",
      "When generating text, we don't need a validation data set.\n",
      "\n",
      "We're going to use every bit we have to try to spot the patterns of where and how words occur.\n",
      "\n",
      "So if we tokenize our entire corpus, there will be, by definition, no out of vocabulary token.\n",
      "\n",
      "However, in a moment you'll see where we will start to pad subsentences from the full corpus, and for that, we'll need some kind of a zero token.\n",
      "\n",
      "Hence, we'll add one here, and counting that token as a valid word.\n",
      "\n",
      "Now that we have a list of sentences and we've tokenized them, we can turn them into a set of training data.\n",
      "\n",
      "Now, there's a key difference here between what we've seen previously for classification and what we'll use for generation.\n",
      "\n",
      "So let's go over this line by line so it's clear.\n",
      "\n",
      "First of all, I'll create an empty list of input sequences.\n",
      "\n",
      "We'll populate this as we go along.\n",
      "\n",
      "Now, for each line in the corpus, we'll create the list of tokens.\n",
      "\n",
      "Note that we're not doing text to sequences for the entire body.\n",
      "\n",
      "We're going to do it one line at a time.\n",
      "\n",
      "So this will give me the text to sequences for the current line.\n",
      "\n",
      "Now for example, this will sequence just the first line the first time through the loop.\n",
      "\n",
      "And \"In the town of Athy one Jeremy Lanigan\" will be tokenized into the numbers as shown.\n",
      "\n",
      "Next, we're going to go through this list and generate n grams from that.\n",
      "\n",
      "What does that mean? It's best if we look at it like this.\n",
      "\n",
      "The line that we tokenized is represented by a list of numbers, but we can split that into a number of other lists.\n",
      "\n",
      "The first two, the first three, the first four, and so on.\n",
      "\n",
      "The reason for that is that we want to train a model to predict the likely next word.\n",
      "\n",
      "So for each sentence we have, we can train it for when you see this word, this one is next.\n",
      "\n",
      "When you see these two words, this one is next.\n",
      "\n",
      "When you see these three words, this one is next, and so on.\n",
      "\n",
      "Now that we've split the sentence into multiple lists, we'll need to pad it.\n",
      "\n",
      "So we'll start by getting the length of the longest of the sentences and then pad everything with a 0 up to the length of the maximum sentence.\n",
      "\n",
      "So now our line of eight words has formed the same seven lists, but each one is now padded with 0's to begin.\n",
      "\n",
      "Thus, we can see our set of input sequences for this one line just looks like this.\n",
      "\n",
      "And this is ideal for giving us features and labels or X's and Y's.\n",
      "\n",
      "We can take everything but the last value as our X, and we can use the last value as our Y.\n",
      "\n",
      "So when we see a bunch of 0's followed by a 4, the label for that will be 2.\n",
      "\n",
      "Similarly, when we see a bunch of 0's ending with a 4 and then a 2, the label for that will be 66.\n",
      "\n",
      "Similarly, 4 to 66 will be labeled as 8.\n",
      "\n",
      "Python makes it super simple for us to slice our lists like this.\n",
      "\n",
      "We can simply use code like this to generate our X's and now our labels.\n",
      "\n",
      "Finally, we'll want our Y to be categorical and one hot encoded, so that when we train, we'll be able to predict across all of the words in our corpus which one is the most likely word to be next in the sequence given the current set of words.\n",
      "\n",
      "And then we can use the keras to categorical to achieve this.\n",
      "\n",
      "So for example, given the above sentence, we'll split it into X and label, where X is the beginning of the list, and the label is 70.\n",
      "\n",
      "We can then 1, hot encode the label to get the Y.\n",
      "\n",
      "And if you look closely, you'll see that the seventieth element in the y list is a one while everything else is 0.\n",
      "\n",
      "So now we have our features and our labels, let's train a neural network with all of the data.\n",
      "\n",
      "And here's a very simple model architecture to achieve that.\n",
      "\n",
      "This is completely unoptimized, particularly in the middle layers, so please feel free to experiment and improve it.\n",
      "\n",
      "It starts with a sequential, adds an embedding at the top like we saw earlier.\n",
      "\n",
      "As there's a massive variation of words, I gave it a lot of dimensions.\n",
      "\n",
      "And in this case, it's 240.\n",
      "\n",
      "The first parameter is the number of unique words in the corpus.\n",
      "\n",
      "The input length is the maximum sequence length minus 1, because we lopped off the final value in each sequence to make a label.\n",
      "\n",
      "After that, we've just got a single LSTM, but we'll make it bi-directional.\n",
      "\n",
      "And then importantly, our output is a dense with the total number of words.\n",
      "\n",
      "Remember that the labels were 1 hot encoded, so we want an output that is representative of this.\n",
      "\n",
      "It's then a matter of defining your loss function and optimizer.\n",
      "\n",
      "Remember, as this is categorical with lots of classes, you'll need a categorical loss function such as categorical cross entropy here.\n",
      "\n",
      "And once you've done that, you just fit the X's to the Y's.\n",
      "\n",
      "As you're training, you might see the initial accuracy is really small like 0.05 or 0.06.\n",
      "\n",
      "Don't worry, it will go up with time.\n",
      "\n",
      "This is very unstructured data, and it's trying to figure out the rules that match your X's to your Y's.\n",
      "\n",
      "When it's done, you'll have a model that you can pass it a sequence, and it will give you the predicted next value.\n",
      "\n",
      "You can use this to then generate poetry, take a sequence, and get the next value, add that to the sequence, pass that to the model, get the next value, add that to the sequence, and so on.\n",
      "\n",
      "With the simple model architecture above, it ends up with an accuracy around 70 to 75%.\n",
      "\n",
      "And that means that given a sequence of words, it will pick the correct word right about 70% of the time.\n",
      "\n",
      "If it gets a sequence of words it hasn't previously seen, it can make a rough prediction for what the next word could be.\n",
      "\n",
      "So to get it to generate text, we can seed it with some words and predict the next value.\n",
      "\n",
      "We'll add that to our string of words and get it to predict the next value, and so on.\n",
      "\n",
      "And here's the code for that.\n",
      "\n",
      "And when seeded with the words, \"I made a poetry machine,\" I got the following sequence generated for me.\n",
      "\n",
      "It's not bad, though if anybody can explain \"shed love raw boo,\" please let me know in the comments below.\n",
      "\n",
      "Experiment with different architectures and run times, and let me know what you come up with.\n",
      "\n",
      "Now, that brings us to the end of this series on NLP.\n",
      "\n",
      "I hope you've enjoyed it, and if you want more, please let us know in the comments, and don't forget to hit that Subscribe button for more great TensorFlow content.\n",
      "\n",
      "Thank you.\n",
      "\n",
      "[MUSIC PLAYING]\n"
     ]
    }
   ],
   "source": [
    "def create_db_and_transcript_from_youtube_video_url(video_url):\n",
    "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
    "    transcript = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents(transcript)\n",
    "\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    return transcript, db\n",
    "\n",
    "video_url = \"https://www.youtube.com/watch?v=ZMudJXhsUpY&list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S&index=6\"\n",
    "transcript, db = create_db_and_transcript_from_youtube_video_url(video_url)\n",
    "\n",
    "# Extract text content from the Document objects and concatenate into a single string\n",
    "transcript_string = ' '.join(doc.page_content for doc in transcript)\n",
    "\n",
    "# Remove '\\n' symbols from the transcript\n",
    "transcript_string = transcript_string.replace('\\n', ' ')\n",
    "\n",
    "# Add appropriate punctuation\n",
    "transcript_string = transcript_string.replace('. ', '.\\n\\n')\n",
    "\n",
    "print(transcript_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ce7ba-4d20-4aaa-a6c1-181c7e8e600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_query(db, query, k=4):\n",
    "    docs = db.similarity_search(query, k=k)\n",
    "    docs_page_content = \" \".join([d.page_content for d in docs])\n",
    "\n",
    "    chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0.2)\n",
    "\n",
    "    # Template to use for the system message prompt\n",
    "    template = \"\"\"\n",
    "        You are a helpful assistant that that can answer questions about youtube videos \n",
    "        based on the video's transcript: {docs}\n",
    "        \n",
    "        Only use the factual information from the transcript to answer the question.\n",
    "        \n",
    "        If you feel like you don't have enough information to answer the question, say \"I don't know\".\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "    # Human question prompt\n",
    "    human_template = \"Answer the following question: {question}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [system_message_prompt, human_message_prompt]\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "    response = chain.run(question=query, docs=docs_page_content)\n",
    "    response = response.replace(\"\\n\", \"\")\n",
    "    return response, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8696c-e369-4c17-9c45-678b1fcb8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "query = \"what is this video about?\"\n",
    "response, docs = get_response_from_query(db, query)\n",
    "print(textwrap.fill(response, width=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecbd836-ed88-478f-9ac4-08de7ac7c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"show me the full transcript\"\n",
    "response, docs = get_response_from_query(db, query)\n",
    "print(textwrap.fill(response, width=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c68d7e-cf77-4f7f-b052-1168e3a212c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what did talk about on AGI?\"\n",
    "response, docs = get_response_from_query(db, query)\n",
    "print(textwrap.fill(response, width=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd46a6bb-ffd6-43d7-9296-44ee92d30f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"On what topic they spend most time?\"\n",
    "response, docs = get_response_from_query(db, query)\n",
    "print(textwrap.fill(response, width=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b768f-b61e-4b4b-96d2-3c5baffb6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"who are the speakers in this video?\"\n",
    "response, docs = get_response_from_query(db, query)\n",
    "print(textwrap.fill(response, width=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da246cd8-6502-4d07-bef6-8337b7facbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are they saying about Microsoft?\"\n",
    "response, docs = get_response_from_query(db, query)\n",
    "print(textwrap.fill(response, width=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380aca57-f648-4083-96e6-a5583902db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4e30b-da83-4d35-94c8-6e562f57380d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
