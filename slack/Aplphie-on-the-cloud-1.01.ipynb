{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25de6c62-2abb-4ed8-a4a8-2f78ca30545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install functions\n",
    "import sys\n",
    "import os\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "from slack_bolt.adapter.flask import SlackRequestHandler\n",
    "from slack_bolt import App\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from flask import Flask, request\n",
    "from functions import draft_email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f308ff91-b161-4621-9d23-cedf3828e4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430dc19b-51ec-4f54-9703-506ad460c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Slack API credentials\n",
    "SLACK_BOT_TOKEN = os.environ[\"SLACK_BOT_TOKEN\"]\n",
    "SLACK_SIGNING_SECRET = os.environ[\"SLACK_SIGNING_SECRET\"]\n",
    "SLACK_BOT_USER_ID = os.environ[\"SLACK_BOT_USER_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfbc499-fa51-438d-8d72-799cf2984908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Slack app\n",
    "app = App(token=SLACK_BOT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894ff6d1-846b-4352-8ae1-69fa4155ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Flask app\n",
    "# Flask is a web application framework written in Python\n",
    "flask_app = Flask(__name__)\n",
    "handler = SlackRequestHandler(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6ee9e1-aaa2-405f-ae30-df8809b10c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bot_user_id():\n",
    "    \"\"\"\n",
    "    Get the bot user ID using the Slack API.\n",
    "    Returns:\n",
    "        str: The bot user ID.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the Slack client with your bot token\n",
    "        slack_client = WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"])\n",
    "        response = slack_client.auth_test()\n",
    "        return response[\"user_id\"]\n",
    "    except SlackApiError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be78566-7771-4c5a-8ddc-c38fcd8c3c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U071FV10PDJ'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bot_user_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04993e2-ddb6-4431-8ead-277eb4cf1fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asifr\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 13.8 MiB for an array with shape (2352, 1536) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Create embeddings and vectorstore\u001b[39;00m\n\u001b[0;32m     25\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[1;32m---> 26\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_texts([text\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts], embeddings)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Define the RetrievalQA chain\u001b[39;00m\n\u001b[0;32m     29\u001b[0m retrieval_qa_chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[0;32m     30\u001b[0m     llm\u001b[38;5;241m=\u001b[39mChatOpenAI(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),  \u001b[38;5;66;03m# Consistency\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m}),  \u001b[38;5;66;03m# Retrieve top 5 results\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     return_source_documents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Enable source documents for debugging\u001b[39;00m\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:931\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    930\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m--> 931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m    932\u001b[0m     texts,\n\u001b[0;32m    933\u001b[0m     embeddings,\n\u001b[0;32m    934\u001b[0m     embedding,\n\u001b[0;32m    935\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    936\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    938\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:900\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    890\u001b[0m index_to_docstore_id \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_to_docstore_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m    891\u001b[0m vecstore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    892\u001b[0m     embedding,\n\u001b[0;32m    893\u001b[0m     index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    899\u001b[0m )\n\u001b[1;32m--> 900\u001b[0m vecstore\u001b[38;5;241m.\u001b[39m__add(texts, embeddings, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vecstore\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:196\u001b[0m, in \u001b[0;36mFAISS.__add\u001b[1;34m(self, texts, embeddings, metadatas, ids)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate ids found in the ids list.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Add to the index.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(embeddings, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_L2:\n\u001b[0;32m    198\u001b[0m     faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(vector)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 13.8 MiB for an array with shape (2352, 1536) and data type float32"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain.document_loaders import UnstructuredEPubLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from slack_bolt import App\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Load the EPUB document\n",
    "epub_loader = UnstructuredEPubLoader(\"Chapter-7-NMSA-1978.epub\")\n",
    "documents = epub_loader.load()\n",
    "\n",
    "# Refine text splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings and vectorstore\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts([text.page_content for text in texts], embeddings)\n",
    "\n",
    "# Define the RetrievalQA chain\n",
    "retrieval_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.2),  # Consistency\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),  # Retrieve top 5 results\n",
    "    return_source_documents=True  # Enable source documents for debugging\n",
    ")\n",
    "\n",
    "# Memory for conversation context\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Define prompt structure for system and human prompts\n",
    "system_template = \"You are a helpful assistant at New Mexico Tax & Rev. Explain Chapter-7-NMSA-1978 clearly and accurately.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_template = \"User asks: {user_input}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "@app.event(\"app_mention\")\n",
    "def handle_mentions(body, say):\n",
    "    try:\n",
    "        event_text = body.get(\"event\", {}).get(\"text\", \"\")\n",
    "        user_input = event_text.split(\">\", 1)[-1].strip()\n",
    "\n",
    "        # Check if the query is about New Mexico Statutes\n",
    "        if \"new mexico statute\" in user_input.lower():\n",
    "            response = retrieval_qa_chain.run(user_input)\n",
    "            # Return the result or a fallback message if no result\n",
    "            say(response['result'] if 'result' in response else \"I couldn't find any information on that topic.\")\n",
    "        else:\n",
    "            # Handle general queries with the same RetrievalQA\n",
    "            response = retrieval_qa_chain.run(user_input)\n",
    "            # Return the result or a fallback message\n",
    "            say(response['result'] if 'result' in response else \"I couldn't generate a response.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error with more detail\n",
    "        print(\"Error handling app_mention:\", e)\n",
    "        # Return a fallback response in case of an error\n",
    "        say(\"Sorry, I encountered an error while processing your request. Could you rephrase or ask again?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6915aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from slack_bolt import App\n",
    "from slack_bolt.adapter.socket_mode import SocketModeHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Initialize the OpenAI-based language model with conversational capabilities\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Create a memory buffer for maintaining context in conversations\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# System prompt template to define AI assistant's behavior\n",
    "system_template = \"\"\"\n",
    "You are Alphie, a conversational AI assistant at New Mexico Tax & Rev. \n",
    "Your role is to engage in meaningful conversations and help with a variety of tasks, including answering general questions, solving coding issues, and more.\n",
    "Provide thoughtful and friendly responses to all user questions. \n",
    "\n",
    "\n",
    "One of your primary tasks is to analyze the Current New Mexico Statutes Annotated 1978. Make sure to do a detailed analysis. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# Human message prompt to integrate user input into the conversation\n",
    "human_template = \"User says: {user_input}.\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Create a conversational prompt with system and human messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# Create a conversational chain with memory for maintaining context\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt, memory=memory)\n",
    "\n",
    "# Define the event listener for Slack messages\n",
    "@app.event(\"message\")\n",
    "def handle_message_events(event, say):\n",
    "    user_text = event.get(\"text\", \"\")\n",
    "\n",
    "    # If the user text is a greeting\n",
    "    if user_text.lower() in [\"hi\", \"hello\", \"hey\"]:\n",
    "        say(\"Hello! I'm Alphie, your friendly AI assistant at New Mexico Tax & Rev. How can I help you today?\")\n",
    "        return\n",
    "\n",
    "    # Generate the response with the conversational chain\n",
    "    response = chain.run(user_input=user_text)\n",
    "\n",
    "    # Respond with the generated response\n",
    "    say(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf9e814-8dc8-48cf-82b2-ae880a310fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flask_app.route(\"/slack/events\", methods=[\"POST\"])\n",
    "def slack_events():\n",
    "    \"\"\"\n",
    "    Route for handling Slack events.\n",
    "    This function passes the incoming HTTP request to the SlackRequestHandler for processing.\n",
    "\n",
    "    Returns:\n",
    "        Response: The result of handling the request.\n",
    "    \"\"\"\n",
    "    return handler.handle(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac3e4b-05aa-4c14-866d-0882678bc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Flask app\n",
    "if __name__ == \"__main__\":\n",
    "    flask_app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27a3ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
