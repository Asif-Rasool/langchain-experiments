{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25de6c62-2abb-4ed8-a4a8-2f78ca30545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install functions\n",
    "import sys\n",
    "import os\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "from slack_bolt.adapter.flask import SlackRequestHandler\n",
    "from slack_bolt import App\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from flask import Flask, request\n",
    "from functions import draft_email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f308ff91-b161-4621-9d23-cedf3828e4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430dc19b-51ec-4f54-9703-506ad460c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Slack API credentials\n",
    "SLACK_BOT_TOKEN = os.environ[\"SLACK_BOT_TOKEN\"]\n",
    "SLACK_SIGNING_SECRET = os.environ[\"SLACK_SIGNING_SECRET\"]\n",
    "SLACK_BOT_USER_ID = os.environ[\"SLACK_BOT_USER_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfbc499-fa51-438d-8d72-799cf2984908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Slack app\n",
    "app = App(token=SLACK_BOT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894ff6d1-846b-4352-8ae1-69fa4155ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Flask app\n",
    "# Flask is a web application framework written in Python\n",
    "flask_app = Flask(__name__)\n",
    "handler = SlackRequestHandler(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6ee9e1-aaa2-405f-ae30-df8809b10c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bot_user_id():\n",
    "    \"\"\"\n",
    "    Get the bot user ID using the Slack API.\n",
    "    Returns:\n",
    "        str: The bot user ID.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the Slack client with your bot token\n",
    "        slack_client = WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"])\n",
    "        response = slack_client.auth_test()\n",
    "        return response[\"user_id\"]\n",
    "    except SlackApiError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04993e2-ddb6-4431-8ead-277eb4cf1fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import find_dotenv, load_dotenv\n",
    "# from langchain.document_loaders import UnstructuredEPubLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores.faiss import FAISS\n",
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from slack_bolt import App\n",
    "# from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv(find_dotenv())\n",
    "\n",
    "# # Load the EPUB document\n",
    "# epub_loader = UnstructuredEPubLoader(\"Chapter-7-NMSA-1978.epub\")\n",
    "# documents = epub_loader.load()\n",
    "\n",
    "# # Refine text splitting\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# # Create embeddings and vectorstore\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# vectorstore = FAISS.from_texts([text.page_content for text in texts], embeddings)\n",
    "\n",
    "# # Define the RetrievalQA chain\n",
    "# retrieval_qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.2),  # Consistency\n",
    "#     retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),  # Retrieve top 5 results\n",
    "#     return_source_documents=True  # Enable source documents for debugging\n",
    "# )\n",
    "\n",
    "# # Memory for conversation context\n",
    "# memory = ConversationBufferMemory()\n",
    "\n",
    "# # Define prompt structure for system and human prompts\n",
    "# system_template = \"You are a helpful assistant at New Mexico Tax & Rev. Explain Chapter-7-NMSA-1978 clearly and accurately.\"\n",
    "# system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "# human_template = \"User asks: {user_input}\"\n",
    "# human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "# chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# @app.event(\"app_mention\")\n",
    "# def handle_mentions(body, say):\n",
    "#     try:\n",
    "#         event_text = body.get(\"event\", {}).get(\"text\", \"\")\n",
    "#         user_input = event_text.split(\">\", 1)[-1].strip()\n",
    "\n",
    "#         # Check if the query is about New Mexico Statutes\n",
    "#         if \"new mexico statute\" in user_input.lower():\n",
    "#             response = retrieval_qa_chain.run(user_input)\n",
    "#             # Return the result or a fallback message if no result\n",
    "#             say(response['result'] if 'result' in response else \"I couldn't find any information on that topic.\")\n",
    "#         else:\n",
    "#             # Handle general queries with the same RetrievalQA\n",
    "#             response = retrieval_qa_chain.run(user_input)\n",
    "#             # Return the result or a fallback message\n",
    "#             say(response['result'] if 'result' in response else \"I couldn't generate a response.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         # Log the error with more detail\n",
    "#         print(\"Error handling app_mention:\", e)\n",
    "#         # Return a fallback response in case of an error\n",
    "#         say(\"Sorry, I encountered an error while processing your request. Could you rephrase or ask again?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4be1c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary modules\n",
    "# import os\n",
    "# from dotenv import find_dotenv, load_dotenv\n",
    "# from langchain.document_loaders import UnstructuredEPubLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores.faiss import FAISS\n",
    "# from langchain.chains import RetrievalQA, LLMChain\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from slack_bolt import App\n",
    "# from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# # Load environment variables (ensure OpenAI and Slack credentials are defined)\n",
    "# load_dotenv(find_dotenv())\n",
    "\n",
    "# # Load the EPUB document containing the New Mexico Statutes\n",
    "# epub_loader = UnstructuredEPubLoader(\"Chapter-7-NMSA-1978.epub\")\n",
    "# documents = epub_loader.load()\n",
    "\n",
    "# # Split text into smaller chunks\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# # Create embeddings for each text chunk\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# # Store the text chunks in a FAISS vectorstore\n",
    "# vectorstore = FAISS.from_texts([text.page_content for text in texts], embeddings)\n",
    "\n",
    "# # Define the RetrievalQA chain\n",
    "# retrieval_qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0),  # Reduced temperature for more consistent responses\n",
    "#     retriever=vectorstore.as_retriever(),\n",
    "#     return_source_documents=False  # Change to True if you want to show the source documents\n",
    "# )\n",
    "\n",
    "# # Memory for maintaining conversation context\n",
    "# memory = ConversationBufferMemory()\n",
    "\n",
    "# # Define a prompt structure for LLMChain\n",
    "# system_template = \"You are a helpful assistant at New Mexico Tax & Rev. You explain Chapter-7-NMSA-1978.\"\n",
    "# system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "# human_template = \"User says: {user_input}.\"\n",
    "# human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "# chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# # Create LLMChain for generic questions\n",
    "# generic_chain = LLMChain(\n",
    "#     llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7),\n",
    "#     prompt=chat_prompt,\n",
    "#     memory=memory\n",
    "# )\n",
    "\n",
    "# # Define the event handler for Slack events\n",
    "# @app.event(\"app_mention\")\n",
    "# def handle_mentions(body, say):\n",
    "#     try:\n",
    "#         # Extract text from the Slack event\n",
    "#         event_text = body.get(\"event\", {}).get(\"text\", \"\")\n",
    "#         user_input = event_text.split(\">\", 1)[-1].strip()  # Strip mention and leading/trailing spaces\n",
    "\n",
    "#         if \"new mexico statute\" in user_input.lower():\n",
    "#             # Use RetrievalQA chain for New Mexico Statute-related queries\n",
    "#             response = retrieval_qa_chain.run(user_input)\n",
    "#             say(response['result'] if 'result' in response else \"I couldn't find any information on that topic.\")\n",
    "#         else:\n",
    "#             # Use LLMChain for general queries\n",
    "#             response = generic_chain.run(user_input=user_input)\n",
    "#             say(response)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         # Handle exceptions\n",
    "#         print(\"Error handling app_mention:\", e)\n",
    "#         say(\"Sorry, I encountered an error while processing your request.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6915aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asifr\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\asifr\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from slack_bolt import App\n",
    "from slack_bolt.adapter.socket_mode import SocketModeHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Initialize the OpenAI-based language model with conversational capabilities\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Create a memory buffer for maintaining context in conversations\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# System prompt template to define AI assistant's behavior\n",
    "system_template = \"\"\"\n",
    "You are Alphie, a conversational AI assistant at New Mexico Tax & Rev. \n",
    "Your role is to engage in meaningful conversations and help with a variety of tasks, including answering general questions, solving coding issues, and more.\n",
    "Provide thoughtful and friendly responses to all user questions. \n",
    "\n",
    "\n",
    "One of your primary tasks is to analyze the Current New Mexico Statutes Annotated 1978. Make sure to do a detailed analysis. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# Human message prompt to integrate user input into the conversation\n",
    "human_template = \"User says: {user_input}.\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Create a conversational prompt with system and human messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# Create a conversational chain with memory for maintaining context\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt, memory=memory)\n",
    "\n",
    "# Define the event listener for Slack messages\n",
    "@app.event(\"message\")\n",
    "def handle_message_events(event, say):\n",
    "    user_text = event.get(\"text\", \"\")\n",
    "\n",
    "    # If the user text is a greeting\n",
    "    if user_text.lower() in [\"hi\", \"hello\", \"hey\"]:\n",
    "        say(\"Hello! I'm Alphie, your friendly AI assistant at New Mexico Tax & Rev. How can I help you today?\")\n",
    "        return\n",
    "\n",
    "    # Generate the response with the conversational chain\n",
    "    response = chain.run(user_input=user_text)\n",
    "\n",
    "    # Respond with the generated response\n",
    "    say(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cf9e814-8dc8-48cf-82b2-ae880a310fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flask_app.route(\"/slack/events\", methods=[\"POST\"])\n",
    "def slack_events():\n",
    "    \"\"\"\n",
    "    Route for handling Slack events.\n",
    "    This function passes the incoming HTTP request to the SlackRequestHandler for processing.\n",
    "\n",
    "    Returns:\n",
    "        Response: The result of handling the request.\n",
    "    \"\"\"\n",
    "    return handler.handle(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac3e4b-05aa-4c14-866d-0882678bc5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [02/May/2024 12:14:09] \"POST /slack/events HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/May/2024 12:14:12] \"POST /slack/events HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/May/2024 12:14:13] \"POST /slack/events HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/May/2024 12:14:22] \"POST /slack/events HTTP/1.1\" 200 -\n",
      "C:\\Users\\asifr\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "127.0.0.1 - - [02/May/2024 12:14:24] \"POST /slack/events HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/May/2024 12:14:31] \"POST /slack/events HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/May/2024 12:14:33] \"POST /slack/events HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/May/2024 12:14:55] \"POST /slack/events HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/May/2024 12:14:59] \"POST /slack/events HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/May/2024 14:51:56] \"POST /slack/events HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/May/2024 14:51:57] \"POST /slack/events HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the Flask app\n",
    "if __name__ == \"__main__\":\n",
    "    flask_app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27a3ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
